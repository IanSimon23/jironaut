{
  "entries": [
    {
      "id": "c7456916-eec7-4bb9-a0db-0d6774f33ff4",
      "timestamp": "2026-01-27T07:50:58.883Z",
      "type": "win",
      "content": "Starting Jironaut 2.0 pivot - from ticket police to ticket coach"
    },
    {
      "id": "0f952cc5-1cbc-4fcc-9c26-d14e524e0b6b",
      "timestamp": "2026-01-27T07:51:51.053Z",
      "type": "note",
      "content": "Created JIRONAUT_V2_PLAN.md capturing architecture, phases, and development workflow"
    },
    {
      "id": "df93cd7c-632b-43b1-bb6e-6919af108117",
      "timestamp": "2026-01-27T08:02:14.983Z",
      "type": "win",
      "content": "Phase 1 complete - Coach mode MVP with chat interface and live ticket preview"
    },
    {
      "id": "a3e1f8b2-7d4c-4a19-b6e0-9c82d1f54a73",
      "timestamp": "2026-01-30T12:00:00.000Z",
      "type": "blocker",
      "content": "Monolithic coach endpoint causing ~5s perceived latency per turn. The single api/coach.js endpoint was doing double duty: generating a conversational coaching response AND extracting structured ticket JSON, all in one synchronous Anthropic call. The model was forced to produce a JSON envelope containing both the chat message and ticket field updates, which meant (1) the entire response had to complete before the user saw anything (~5s of staring at 'Thinking...'), (2) the JSON output constraint made the coaching voice formulaic and robotic — temperature 0.4 with a rigid JSON schema ate all the personality, (3) token budget of 512 was split between coaching prose and JSON overhead so responses were often truncated, and (4) JSON parse failures from the model occasionally broke the whole turn with no fallback. Resolution: split into two concerns. api/coach.js rewritten as a Vercel Edge Function with SSE streaming — plain text only, temperature 0.7, 1024 token budget, rich system prompt with the full guided prompting framework, few-shot examples, and anti-parrot rules. Text starts appearing in ~0.5-1s via ReadableStream. New api/extract.js runs in the background after the coaching response finishes — Haiku at temperature 0, focused extraction prompt, cumulative across the full conversation so it self-corrects. Extraction failures are non-fatal; the conversation continues and the next turn re-extracts. Frontend hook rewritten with getReader() stream consumption, placeholder message with live content updates, pulsing cursor during stream, and input gating until stream completes."
    },
    {
      "id": "b7d23a91-4e5f-4c8b-a1d3-6f09e2c87b45",
      "timestamp": "2026-01-30T12:30:00.000Z",
      "type": "blocker",
      "content": "SSE streaming implemented but responses still arriving in one 50s burst. The coach endpoint was correctly calling res.write() for each chunk, but vercel dev buffers the entire serverless function response before forwarding it to the browser — the streaming was working server-side but the dev proxy held everything until res.end(). Additionally, the initial Edge Function approach (export const config = { runtime: 'edge' }) failed with an esbuild cross-platform binary error — node_modules was installed on Windows but WSL needs the Linux esbuild binary. Resolution: (1) Reverted api/coach.js from Edge Function back to standard Node.js serverless function using res.write() — same SSE protocol, no esbuild dependency. (2) Added a Vite plugin (apiMiddleware) in vite.config.js that handles /api/* routes directly inside Vite's dev server, bypassing vercel dev entirely. The plugin parses JSON bodies, polyfills Vercel's res.status() and res.json() helpers, loads handlers via server.ssrLoadModule() for HMR support, and loads .env vars via loadEnv() with empty prefix so ANTHROPIC_API_KEY is available on process.env. Raw Node.js HTTP res.write() flushes immediately — no intermediate proxy to buffer. Dev now runs via npm run dev (Vite) instead of vercel dev. Production Vercel deploys still use the serverless functions unchanged."
    },
    {
      "id": "e4a82c17-9f31-4d6a-b5e8-3a17df0b9c62",
      "timestamp": "2026-01-30T13:00:00.000Z",
      "type": "note",
      "content": "Ticket Preview panel was barely populating despite rich streaming conversation. Two root causes: (1) useTicketBuilder was using append-only logic for successCriteria and constraints arrays (Set-based dedup + spread), but the extraction endpoint is cumulative — it returns the full picture from the entire conversation each time. When the model rephrased between turns, near-duplicate entries accumulated. Fixed by switching to full replacement on each extraction call. (2) The extraction prompt in api/extract.js was too thin — no field-level definitions, no example, vague section advancement rules. Haiku was guessing what 'intent' vs 'outcome' meant in natural conversation context and often returning nulls or weak extractions. Rewrote with explicit field definitions (length, focus, what to include), a concrete conversation-to-JSON example, clear advancement rules (advance when coach has moved on, constraints is optional, complete when 4 core fields populated), and bumped max_tokens from 256 to 512 since cumulative extraction of a full ticket was hitting the token ceiling and truncating mid-JSON."
    },
    {
      "id": "f1c74d03-8b29-4e52-a9f7-2d61e3b08c4a",
      "timestamp": "2026-01-30T13:30:00.000Z",
      "type": "win",
      "content": "Streaming coach merged to main and deployed to production. Coaching responses now stream in word-by-word (~0.5s to first character vs ~5s before), conversation quality is rich and natural thanks to the full guided prompting framework with few-shot examples, and ticket extraction runs in the background without blocking the conversation. Three blockers hit and resolved in one session: monolithic endpoint split into streaming coach + async extraction, vercel dev buffering bypassed with a Vite API middleware plugin, and extraction quality fixed with field-level prompt definitions and cumulative replacement logic."
    },
    {
      "id": "d8b45e21-3a7c-4f19-9e12-7c06a4d83f19",
      "timestamp": "2026-01-30T14:00:00.000Z",
      "type": "blocker",
      "content": "Live demo feedback: ticket preview panel never populated, conversation felt like it was 'dragging it out', and someone asked 'why is this better than ChatGPT?' Two root causes found. (1) The messages array sent to both API endpoints started with role:'assistant' (the initial UI greeting). Anthropic's Messages API requires the first message to have role:'user'. The coach streaming was apparently tolerant of this, but the extraction endpoint was silently failing — the runExtraction catch block swallowed errors with no logging, so the ticket panel sat empty the entire demo. Fixed by adding a toApiMessages() filter that strips the initial greeting before any API call, and added actual error logging on extraction failure. (2) The coaching prompt was too thorough — 5 detailed sections with optional nudges, coaching cues, and anti-patterns led to 10+ exchanges per ticket. Each section got follow-up questions even when the user's answer was clear. Rewrote the system prompt targeting 4-6 total exchanges: one question per response, 2-3 sentences max, skip sections when answers cover multiple at once, constraints is one question that accepts 'no' immediately. Replaced the verbose framework prose with a compact end-to-end example showing the full flow in 5 exchanges. The ticket panel IS the product differentiator — without it working, there's no answer to 'why not just use ChatGPT.'"
    },
    {
      "id": "c9f18a35-2d67-4b03-8e4a-5a92c0df71b8",
      "timestamp": "2026-01-30T14:30:00.000Z",
      "type": "blocker",
      "content": "Testing with demo scenario 1a (dashboard search): extraction populated intent on first turn but ticket panel stalled after that — outcome, scope, success, constraints all stayed empty. Coach eventually wrote out a full formatted ticket in the chat itself, which defeated the entire purpose of the ticket panel. Three fixes deployed. (1) Added explicit rule to coach prompt: 'NEVER draft, write out, summarize, or format a ticket. The ticket builds automatically in the panel. Your ONLY job is asking questions.' (2) Section advancement was entirely dependent on the extractor's suggestedSection field, which was conservative and got stuck at 'outcome' even when the conversation had covered multiple sections. Replaced with data-driven advancement: deriveSectionFromData() checks which fields are actually populated (has intent → outcome, has intent+outcome → scope, etc.) and takes the furthest of either the data signal or the extractor's suggestion. The panel can no longer get stuck if the extractor is behind. (3) The two signals are now composed with furthestSection() — whichever is ahead wins — so extraction quality issues can't hold back the UI."
    },
    {
      "id": "a2c43b78-6e19-4d5a-b7f1-8d20e9a35c14",
      "timestamp": "2026-01-30T15:00:00.000Z",
      "type": "blocker",
      "content": "Coach still re-asking covered sections despite 'be fast' prompt. Testing scenario 1a again: user said '1-2 second response times, reduce errors and support tickets' (covers outcome AND success) but the coach asked 'what tangible difference will users experience beyond faster results?' — re-probing a section that was clearly answered. User said 'none' and the coach accepted but then asked about scope, user said 'everything', and the coach asked 'what would you explicitly NOT want the team to spend time on?' — re-asking the same section in reverse. Root cause: the system prompt was structured as advisory framework prose that the model treated as suggestions, and 'CURRENT SECTION: outcome' with 'Focus your coaching on this section' was anchoring the model to keep probing the named section. Fixed by rewriting the prompt as blunt direct rules instead of framework guidance, adding explicit BAD examples showing the exact failure patterns observed (re-asking after clear answers, pushing back on 'none', re-asking scope when user said 'everything'), and softening the CURRENT SECTION instruction from 'Focus on this section' to 'Pick up from here. If already covered, move to next.' Key learning: for small fast models like Haiku, negative examples ('NEVER DO THIS') are more effective than positive style rules at preventing unwanted behaviour."
    },
    {
      "id": "b3f59c12-7a84-4d2e-9e16-4c08a1f52d37",
      "timestamp": "2026-01-30T15:30:00.000Z",
      "type": "blocker",
      "content": "Coach prompt quality much improved after negative-example rewrite — onboarding scenario completed in 5 exchanges with natural, specific questions. However the ticket panel only populated at the very end of the conversation — intent appeared on the first turn but outcome, scope, success, and constraints all filled in a burst 2-3 seconds after the final coaching exchange. Root cause: the extraction prompt was conservative by design. It had a CURRENT SECTION anchor and section advancement rules that made it wait for each section to be formally discussed before extracting its field. If the user mentioned scope while answering about outcome, the extractor ignored it because it was 'still on outcome.' The entire right-hand panel sat empty through the middle of the conversation, which undermines the core differentiator — the user should see the ticket building in real time as they talk. Resolution: rewrote the extraction prompt from conservative to aggressive. Removed the CURRENT SECTION anchor entirely. Added CRITICAL RULES: 'Extract a field AS SOON as any relevant information appears, even if the conversation hasn't formally reached that section', 'If the user mentions scope while answering about outcome, extract scope', 'Do not wait for a section to be discussed — extract what's there.' Added a new example showing scope extracted from an outcome answer because the user's response implied it. Trimmed the prompt overall to reduce token overhead and improve Haiku compliance. The extractor should now populate fields progressively through the conversation rather than waiting until the end."
    },
    {
      "id": "e7a21f94-3c56-4b8e-a2d7-9f14b0c63e58",
      "timestamp": "2026-01-31T09:00:00.000Z",
      "type": "note",
      "content": "Extraction still not populating fields early enough despite 'extract everything' rewrite. Intent and scope populated but outcome didn't appear until later even though it was inferable after the second exchange, and success criteria / constraints could have started populating after the third. The extractor was still waiting for explicit statements rather than inferring from implications. Rewrote the extraction prompt with an inference-first philosophy: 'A rough extraction now is better than a perfect one later — you get the full conversation again next turn to refine.' Added rule: 'Extract from hints and implications, not just explicit statements. If the problem implies a solution, extract outcome.' Replaced the single example with two progressive examples: Example 1 shows 3 fields extracted from a single opening message (intent explicit, outcome and scope both inferred from the problem description), Example 2 shows successCriteria inferred from an outcome answer containing concrete numbers ('1-2 seconds', 'reduce error logs'). The key shift is from 'extract what's stated' to 'extract what can be reasonably inferred' — the ticket panel should fill progressively as the conversation unfolds, giving the user a chance to correct fields early rather than seeing everything appear at the end."
    },
    {
      "id": "f4b82d16-9e53-4a7c-b1f0-6c23a8d95e41",
      "timestamp": "2026-01-31T09:52:00.000Z",
      "type": "win",
      "content": "Tested the 'acceptance criteria overload' anti-pattern scenario (demo scenario 4c): user opened with pixel-spec success criteria ('button is blue #0066CC, 42px tall, 16px padding, Inter font...') before stating any intent or outcome. Coach handled it well — didn't get distracted by the specs, redirected to motivation ('What's the motivation behind changing the landing page button?'), completed in 5 exchanges with one question per turn, accepted answers and moved on. Coaching rated 8/10: minor dings for slightly over-engineered phrasing on the outcome question and could have been 4 exchanges since 'it doesn't look right' plus the opening already covered intent. Extraction rated 9/10: correctly separated pixel specs into success criteria (where they belong) rather than stuffing them into intent or outcome, synthesized intent from two separate answers rather than copying verbatim, wrote outcome about user perception not implementation tasks, captured design guidelines PDF as both success criterion and constraint, kept scope tight (button in, page redesign out). Only miss: 'out of scope' could have explicitly listed the general page overhaul the user said was 'to follow' — a clear deferral signal. Panel didn't populate until the end, but fair for this scenario — the user front-loaded success criteria but didn't give intent or outcome until prompted, so progressive extraction couldn't reasonably infer 'professional first impression' from 'it doesn't look right.' The final ticket output is genuinely useful: someone picking it up would know why, what good looks like, what to build, and what to check. The value prop is working."
    },
    {
      "id": "a1d73e48-5f92-4c1b-b8a0-7e36d2f19c05",
      "timestamp": "2026-01-31T10:15:00.000Z",
      "type": "win",
      "content": "Added 'Copy to Jira' button to the ticket preview panel. Outputs Jira Cloud markdown with ### headings, **bold** scope labels, - [ ] checkboxes for success criteria (become interactive task lists when pasted into Jira Cloud description), and regular bullets for constraints. Button disabled until at least intent is populated, shows 'Copied for Jira' with green check icon for 2 seconds after click. Replaces the 'Format options coming soon' placeholder that was there since Phase 1. This closes the loop on the core workflow: coach conversation → live ticket preview → copy to Jira. The success criteria checkboxes are the highlight — devs can tick them off as they work through the ticket."
    },
    {
      "id": "d2e84f19-7b36-4a5c-9d01-3f18c6a72e90",
      "timestamp": "2026-01-31T10:45:00.000Z",
      "type": "win",
      "content": "Created standalone TICKET_COACH_PROMPT.md — a single portable markdown file that encapsulates the entire coaching and extraction logic for use in any AI chat tool (ChatGPT custom instructions, Claude projects, GitHub Copilot, etc.). Merges the battle-tested coach.js prompt (conversation rules, good/bad examples, section pacing) with the extract.js logic (inference from hints, progressive field tracking) and the Jira Cloud markdown output format (### headings, - [ ] checkboxes for success criteria). Key insight: people may not want to use the Jironaut frontend — they'll want the prompting methodology in their existing tools. This makes the coaching framework portable. Someone can drop the file into any project and get the same ticket quality without the app."
    },
    {
      "id": "c8f41a23-6d19-4e7b-a5c0-2b93e7d08f16",
      "timestamp": "2026-01-31T11:15:00.000Z",
      "type": "note",
      "content": "Reworked header branding to match writing-gym pattern. Removed separate h1 title and tagline text, replaced with a larger standalone logo on the left (h-14, auto width). Moved coach/classic toggle slightly left, added a vertical divider, then GitHub icon linking to the repo and Curious Coach Tools logo linking to curiouscoach.tools. Tightened header padding for a compact look. Consistent branding across both tools now."
    },
    {
      "id": "b5c92d47-8a13-4f6e-c3b1-4d27e8f01a93",
      "timestamp": "2026-01-31T11:30:00.000Z",
      "type": "note",
      "content": "Tweaked header logo sizing — bumped from h-14 to h-[4.8rem] (~37% larger) for better visual presence. Kept compact py-3 padding after briefly adding more whitespace and rolling it back. Added both logo PNGs to git (jironaut-logo.png updated, curious_coach_tools_logo.png new). Left .xcf GIMP source file out of the repo."
    },
    {
      "id": "a9e26f81-4c53-4d7a-b2e0-8f15c3a49d72",
      "timestamp": "2026-01-31T11:45:00.000Z",
      "type": "note",
      "content": "Fixed page loading scrolled down — browser was restoring previous scroll position on refresh, chopping off the top of the logo. Added window.scrollTo(0, 0) on App mount to force scroll to top."
    }
  ]
}